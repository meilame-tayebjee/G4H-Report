[
  {
    "objectID": "mydocs/csi_slides/index.html#about-me",
    "href": "mydocs/csi_slides/index.html#about-me",
    "title": "PhD Committee",
    "section": "About me",
    "text": "About me\n\nGraduated from HEC (19-24), X (X20), ENSAE and MVA (administrateur de l’Insee, 2024)\nResearch internships:\n\n2023 (3A X): MIT (Operations Research Center), Microtransit system design and optimization\n2024 (ENSAE-MVA): Inria HeKA, Generative models for longitudinal images\n\nCurrently, first position in the Corps since sept. 2024:\n\nData Scientist at SSP Lab (MLOps, model trustworthiness, NLP (text classification, RAG), satellite imagery…)\nPhD in CS Candidate (part-time, journées recherche)"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#broader-view-of-the-graph4health-anr",
    "href": "mydocs/csi_slides/index.html#broader-view-of-the-graph4health-anr",
    "title": "PhD Committee",
    "section": "Broader view of the Graph4Health ANR",
    "text": "Broader view of the Graph4Health ANR\n\nThe general objective is to\n\ninfer the causal impact of the healthcare supply (and its geographic distribution) on health outcomes\n\nusing classic econometric tools\nusing deep learning methods for causal inference\n\nstudy policies that contribute to the efficiency of the French healthcare system\n\n10 years of exhaustive SNDS\n\n70M patients\nconsultations, procedures, hospitalizations, medications, deaths etc."
  },
  {
    "objectID": "mydocs/csi_slides/index.html#my-project-embedding-patient-pathways-for-causal-inference",
    "href": "mydocs/csi_slides/index.html#my-project-embedding-patient-pathways-for-causal-inference",
    "title": "PhD Committee",
    "section": "My project: embedding patient pathways for causal inference",
    "text": "My project: embedding patient pathways for causal inference"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#transformer-based-models-for-medical-pathway-embedding",
    "href": "mydocs/csi_slides/index.html#transformer-based-models-for-medical-pathway-embedding",
    "title": "PhD Committee",
    "section": "Transformer-based models for medical pathway embedding",
    "text": "Transformer-based models for medical pathway embedding\n\n\n\nTable 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel name\nModel archi.\nPre-train. Task(s)\nFine-tun. task(s)\n# Params\nTraining data size\nVocab size (type)\nGenerative\nTrajectory1\n\n\n\n\nDelphi (Shmatko et al. (2025))\nNanoGPT\nNext disease event + Time to event\nNone\n2M\n400k patients\n1,258 (ICD10)\n✅\n✅\n\n\nMOTOR (Many Outcome Time Oriented Representation) (Steinberg et al. (2024))\nTransformer (causal masking and local attention)\nTime to event (TTE)\nTTE for part. disease\n143M\n55M patients, 9B events\n8,192 (ICD10 and other specific codes)\n≅\n≅\n\n\nLife2Vec (Savcisens et al. (2023))\nBERT\nMLM + SOP\nMortality Prediction\n8M\n3M patients\n~700(labour + ICD10)\n❌\n❌\n\n\nTransformEHR (Yang et al. (2023))\nEncoder-Decoder\nVisit masking: Given visits 1-3, predict all codes in visit 4\nDisease (pancreatic cancer) / Outcome (self harm) pred.\n?\n255M visits from 7M patients\nICD10\n✅\n❌\n\n\nBEHRT (Li et al. (2020))\nBERT\nMLM\nDisease prediction\nnot reported (~6L, 12H, h=288)\n1.6M patients\n301 disease codes\n❌\n❌\n\n\n\n\n\n\n\nEfficient (i.e. in one forward pass) computation of embeddings at any time"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#mldl-for-causal-inference",
    "href": "mydocs/csi_slides/index.html#mldl-for-causal-inference",
    "title": "PhD Committee",
    "section": "ML/DL for causal inference",
    "text": "ML/DL for causal inference\nSeveral methodological papers:\n\nAbécassis et al. (2025), Doutreligne and Varoquaux (2025)\n\nThe best predictive model is not the best suited for causal inference\nModel selection using risk scores\nDesigning a causal inference framework (DAG, PICOT)\n\nWager (2024): reading group"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#causal-impact-of-pcp-loss-on-health-outcomes---wip",
    "href": "mydocs/csi_slides/index.html#causal-impact-of-pcp-loss-on-health-outcomes---wip",
    "title": "PhD Committee",
    "section": "Causal impact of PCP loss on health outcomes - WIP",
    "text": "Causal impact of PCP loss on health outcomes - WIP"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#data-management",
    "href": "mydocs/csi_slides/index.html#data-management",
    "title": "PhD Committee",
    "section": "Data management",
    "text": "Data management\n\nValidation and EDA of the first delivered simplified tables: prestations and medications\nJointure, cleaning and subsampling\n\nSo that any subsample can fit in vRAM\n\nTokenization\n\nIMAGE of data"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#notations",
    "href": "mydocs/csi_slides/index.html#notations",
    "title": "PhD Committee",
    "section": "Notations",
    "text": "Notations\nNotations: dataset\nDataset of medical pathways:  For i = 1, \\dots, n, X_i \\coloneqq (e_i, t_i, f_i), where:\n\ne_i \\in \\mathbb{R}^{cs} are the tokenized events, containing integers between 0 and the vocabulary size \\lvert \\mathcal{V} \\rvert\nt_i \\in \\mathbb{R}^{cs} is the temporal vector, containing the dates\nf_i \\in \\mathbb{R}^{2} corresponds to the patient general features, for now age and gender"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#pre-training-task",
    "href": "mydocs/csi_slides/index.html#pre-training-task",
    "title": "PhD Committee",
    "section": "Pre-training task",
    "text": "Pre-training task\nObjectives\n\nHaving a time-aware Transformers (\\neq NLP)\nKeeping in mind causal inference\n\nWe want to be able to efficiently compute the embedding of the pathway at any time (the trajectory of the patient)\nIdeally we want to have a generative model - to generate counterfactuals"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#first-trained-models",
    "href": "mydocs/csi_slides/index.html#first-trained-models",
    "title": "PhD Committee",
    "section": "First trained models",
    "text": "First trained models\nWe trained a “large” model (40M params) and a “small” one (5M).\n\nROC curve of the pre-training task for a given specialty on a test set of 10,000 patients.Still, the models converge quickly, and even small models achieve good convergence. This raises the (open) question of complexifying the task at some point."
  },
  {
    "objectID": "mydocs/csi_slides/index.html#plot-of-the-embedding-space---wip",
    "href": "mydocs/csi_slides/index.html#plot-of-the-embedding-space---wip",
    "title": "PhD Committee",
    "section": "Plot of the embedding space - WIP",
    "text": "Plot of the embedding space - WIP"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#causal-inference",
    "href": "mydocs/csi_slides/index.html#causal-inference",
    "title": "PhD Committee",
    "section": "Causal inference",
    "text": "Causal inference\n\nGoal: Estimate the impact of losing one’s Médecin Traitant (Primary Care Provider, PCP), and looping back with Titouan’s work\n\n\nUp to a given time t, we have:\n\nThe covariate X_{&lt;t} \\coloneqq (e_{&lt;t}, t_{&lt;t}, f_{&lt;t}), the medical history of the patient\nW_t is the treatment at time t, say: the proportion of days the patient has spent without a PCP during the period [t- \\delta_{PCP}, t], \\delta_{PCP} being a hyperparameter controlling the length of the window\nY_t (W_t), the binary outcome, indicating whether death has occured within a given time window [t, t+ \\delta_d](which length \\delta_d is again chosen)"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#benchmarking",
    "href": "mydocs/csi_slides/index.html#benchmarking",
    "title": "PhD Committee",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nHow to compare embedding models ? What is a good embedding ?\n\nDeath prediction\nR-loss in a causal inference context (Doutreligne and Varoquaux (2025))\n\nCompetitors:\n\nSVD-PPMI (static embeddings)\nBERT (pure embeddings, no trajectories)\n\nMLM pre-training task (Savcisens et al. (2023))\nTemporal window masking task\n\nGPT with next-token + time-to-event task (Shmatko et al. (2025))"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#improvement-of-the-pre-training-task---wip",
    "href": "mydocs/csi_slides/index.html#improvement-of-the-pre-training-task---wip",
    "title": "PhD Committee",
    "section": "Improvement of the pre-training task - WIP",
    "text": "Improvement of the pre-training task - WIP\n\nGoing generative"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#scaling---wip",
    "href": "mydocs/csi_slides/index.html#scaling---wip",
    "title": "PhD Committee",
    "section": "Scaling - WIP",
    "text": "Scaling - WIP\n\nChinchilla paper: scaling law"
  },
  {
    "objectID": "mydocs/csi_slides/index.html#section",
    "href": "mydocs/csi_slides/index.html#section",
    "title": "PhD Committee",
    "section": "",
    "text": "Abécassis, Judith, Élise Dumas, Julie Alberge, and Gaël Varoquaux. 2025. ‘From prediction to prescription: Machine learning and Causal Inference’. Annual Review of Biomedical Data Science, April. https://doi.org/10.1146/annurev-biodatasci-103123-095750.\n\n\nDoutreligne, Matthieu, and Gaël Varoquaux. 2025. ‘How to Select Predictive Models for Decision-Making or Causal Inference’. GigaScience 14 (January): giaf016. https://doi.org/10.1093/gigascience/giaf016.\n\n\nLi, Yikuan, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy, Yajie Zhu, Kazem Rahimi, and Gholamreza Salimi-Khorshidi. 2020. ‘BEHRT: Transformer for Electronic Health Records’. Scientific Reports 10 (1): 7155. https://doi.org/10.1038/s41598-020-62922-y.\n\n\nSavcisens, Germans, Tina Eliassi-Rad, Lars Kai Hansen, Laust Hvas Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, and Sune Lehmann. 2023. ‘Using Sequences of Life-Events to Predict Human Lives’. Nature Computational Science 4 (1): 43–56. https://doi.org/10.1038/s43588-023-00573-5.\n\n\nShmatko, Artem, Alexander Wolfgang Jung, Kumar Gaurav, Søren Brunak, Laust Hvas Mortensen, Ewan Birney, Tom Fitzgerald, and Moritz Gerstung. 2025. ‘Learning the Natural History of Human Disease with Generative Transformers’. Nature, September. https://doi.org/10.1038/s41586-025-09529-3.\n\n\nSteinberg, Ethan, Jason Alan Fries, Yizhe Xu, and Nigam Shah. 2024. ‘MOTOR: A Time-to-Event Foundation Model for Structured Medical Records’. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=NialiwI2V6.\n\n\nWager, Stefan. 2024. Causal Inference: A Statistical Learning Approach.\n\n\nYang, Zhichao, Avijit Mitra, Weisong Liu, Dan Berlowitz, and Hong Yu. 2023. ‘TransformEHR: Transformer-Based Encoder-Decoder Generative Model to Enhance Prediction of Disease Outcomes Using Electronic Health Records’. Nature Communications 14 (1): 7857. https://doi.org/10.1038/s41467-023-43715-z."
  },
  {
    "objectID": "mydocs/technical_report/index.html",
    "href": "mydocs/technical_report/index.html",
    "title": "G4H - From embeddings to causal inference",
    "section": "",
    "text": "We consider a dataset (X_i)_{i = 1..n} of medical pathways. For each patient i, X_i \\coloneqq (e_i, t_i, f_i), where:\n\ne_i \\in \\mathbb{R}^{cs} denotes the vector of tokenized events. It is a tensor1 which size is common for all patients2 (an hyperparameter called the context size, cs), that contains integers between 0 and the vocabulary size \\lvert \\mathcal{V} \\rvert.\nt_i \\in \\mathbb{R}^{cs} is the temporal vector, containing the dates3 for each event in e_i. For now we only consider consultation and medication type of events.\nf_i \\in \\mathbb{R}^{2} corresponds to the patient general features, for now age and gender.\n\nAt this stage, the pathways only contain consultations and pharmaceuticals events. We plan to add way more types of event, including deaths, hospitalizations, medical procedures etc.\n\n\n\n\n\n\nAn embedding matrix of size \\mathbb{R}^{\\lvert \\mathcal{V} \\rvert, d_{embed}} projects each token of the sequence to an embedding vector. Adapting from the classical Transformer, we use t_i to have a time-aware positional encoding using a sinusoidal encoding. At the beginning of the sequence, we add a symbolic [START] token, composed of the summed embeddings of age and gender.\nThese embeddings are then passed to causal4 self-attention layers, so that the Transformer-based model outputs causally contextualized embeddings for each token:  GPT_{\\theta}(X_i) \\in \\mathbb{R}^{cs, d_{embed}} where \\theta parametrizes the neural network.\n\n\n\n\nAt a given position in the pathway, we consider the pre-training task of predicting, for each token, if it will appear within the next t days. We consider, simulatenously, several t \\in \\mathcal{T}, \\mathcal{T} being the sorted set of short term horizons, typically 14, 30, 60, 90, 180 days.\nLet, for this task:\n\ng_{\\phi} \\colon \\mathbb{R}^{cs, d_{embed}} \\to [0, 1]^{cs, \\lvert \\mathcal{V} \\rvert, \\lvert \\mathcal{T} \\rvert } the neural prediction head for this task (parametrized by \\phi), on top of the Transformer layers\nfor a patient i, Y_{i, \\cdot, \\cdot, \\cdot} \\in \\{ 0, 1 \\}^{cs, \\lvert \\mathcal{V} \\rvert, \\lvert \\mathcal{T} \\rvert} the ground truth5\n\nThe pre-training solves:\n\\begin{align*}\n\\theta^{*}, \\phi^{*} \\in \\arg\\!\\min \\; & \\sum_{i=1}^{n} \\sum_{\\text{pos}=1}^{cs} \\sum_{w \\in \\mathcal{V}} \\sum_{t \\in \\mathcal{T}}\n\\text{BCE}\\left(\n    g_{\\phi} \\circ GPT_{\\theta}(X_i)_{\\text{pos}, w, t},\n    Y_{i, \\text{pos}, w, t}\n\\right) \\\\\n& + \\lambda \\, \\text{Mon}\\left( g_{\\phi} \\circ GPT_{\\theta}(X_i) \\right)\n\\end{align*}\nwhere:\n\n\\text{BCE}(x,y) = y \\log(x) + (1-y) \\log(1-x)\nfor all i, \\text{Mon}\\left( g_{\\phi} \\circ GPT_{\\theta}(X_i) \\right) \\coloneqq \\sum_{t, t' | t' &gt; t} \\max \\left(0, g_{\\phi} \\circ GPT_{\\theta}(X_i)_{\\cdot, \\cdot, t} - GPT_{\\theta}(X_i)_{\\cdot, \\cdot, t'} \\right)\n\n\n\n\nWe trained a “large” model (40M parameters) and a “small” one (5M parameters). It seems that the big model improves the result, leading us to think there is a signal to capture and that the task is “learnable”.\n\n\n\nROC curve of the pre-training task for a given specialty on a test set of 10,000 patients.\n\n\nStill, the models converge quickly, and even small models achieve good convergence. This raises the (open) question of complexifying the task at some point."
  },
  {
    "objectID": "mydocs/technical_report/index.html#data",
    "href": "mydocs/technical_report/index.html#data",
    "title": "G4H - From embeddings to causal inference",
    "section": "",
    "text": "We consider a dataset (X_i)_{i = 1..n} of medical pathways. For each patient i, X_i \\coloneqq (e_i, t_i, f_i), where:\n\ne_i \\in \\mathbb{R}^{cs} denotes the vector of tokenized events. It is a tensor1 which size is common for all patients2 (an hyperparameter called the context size, cs), that contains integers between 0 and the vocabulary size \\lvert \\mathcal{V} \\rvert.\nt_i \\in \\mathbb{R}^{cs} is the temporal vector, containing the dates3 for each event in e_i. For now we only consider consultation and medication type of events.\nf_i \\in \\mathbb{R}^{2} corresponds to the patient general features, for now age and gender.\n\nAt this stage, the pathways only contain consultations and pharmaceuticals events. We plan to add way more types of event, including deaths, hospitalizations, medical procedures etc."
  },
  {
    "objectID": "mydocs/technical_report/index.html#transformer-based-models",
    "href": "mydocs/technical_report/index.html#transformer-based-models",
    "title": "G4H - From embeddings to causal inference",
    "section": "",
    "text": "An embedding matrix of size \\mathbb{R}^{\\lvert \\mathcal{V} \\rvert, d_{embed}} projects each token of the sequence to an embedding vector. Adapting from the classical Transformer, we use t_i to have a time-aware positional encoding using a sinusoidal encoding. At the beginning of the sequence, we add a symbolic [START] token, composed of the summed embeddings of age and gender.\nThese embeddings are then passed to causal4 self-attention layers, so that the Transformer-based model outputs causally contextualized embeddings for each token:  GPT_{\\theta}(X_i) \\in \\mathbb{R}^{cs, d_{embed}} where \\theta parametrizes the neural network.\n\n\n\n\nAt a given position in the pathway, we consider the pre-training task of predicting, for each token, if it will appear within the next t days. We consider, simulatenously, several t \\in \\mathcal{T}, \\mathcal{T} being the sorted set of short term horizons, typically 14, 30, 60, 90, 180 days.\nLet, for this task:\n\ng_{\\phi} \\colon \\mathbb{R}^{cs, d_{embed}} \\to [0, 1]^{cs, \\lvert \\mathcal{V} \\rvert, \\lvert \\mathcal{T} \\rvert } the neural prediction head for this task (parametrized by \\phi), on top of the Transformer layers\nfor a patient i, Y_{i, \\cdot, \\cdot, \\cdot} \\in \\{ 0, 1 \\}^{cs, \\lvert \\mathcal{V} \\rvert, \\lvert \\mathcal{T} \\rvert} the ground truth5\n\nThe pre-training solves:\n\\begin{align*}\n\\theta^{*}, \\phi^{*} \\in \\arg\\!\\min \\; & \\sum_{i=1}^{n} \\sum_{\\text{pos}=1}^{cs} \\sum_{w \\in \\mathcal{V}} \\sum_{t \\in \\mathcal{T}}\n\\text{BCE}\\left(\n    g_{\\phi} \\circ GPT_{\\theta}(X_i)_{\\text{pos}, w, t},\n    Y_{i, \\text{pos}, w, t}\n\\right) \\\\\n& + \\lambda \\, \\text{Mon}\\left( g_{\\phi} \\circ GPT_{\\theta}(X_i) \\right)\n\\end{align*}\nwhere:\n\n\\text{BCE}(x,y) = y \\log(x) + (1-y) \\log(1-x)\nfor all i, \\text{Mon}\\left( g_{\\phi} \\circ GPT_{\\theta}(X_i) \\right) \\coloneqq \\sum_{t, t' | t' &gt; t} \\max \\left(0, g_{\\phi} \\circ GPT_{\\theta}(X_i)_{\\cdot, \\cdot, t} - GPT_{\\theta}(X_i)_{\\cdot, \\cdot, t'} \\right)\n\n\n\n\nWe trained a “large” model (40M parameters) and a “small” one (5M parameters). It seems that the big model improves the result, leading us to think there is a signal to capture and that the task is “learnable”.\n\n\n\nROC curve of the pre-training task for a given specialty on a test set of 10,000 patients.\n\n\nStill, the models converge quickly, and even small models achieve good convergence. This raises the (open) question of complexifying the task at some point."
  },
  {
    "objectID": "mydocs/technical_report/index.html#framework",
    "href": "mydocs/technical_report/index.html#framework",
    "title": "G4H - From embeddings to causal inference",
    "section": "2.1 Framework",
    "text": "2.1 Framework\nUp to a given time t, we have:\n\nThe covariate X_{&lt;t} \\coloneqq (e_{&lt;t}, t_{&lt;t}, f_{&lt;t}), the medical history of the patient\nW_t is the treatment at time t and can either be:\n\nthe proportion of days the patient has spent without a PCP during the period [t- \\delta_{PCP}, t], \\delta_{PCP} being a hyperparameter controlling the length of the window\nthe maximum number of consecutive days without having a PCP, anytime in the past\n\nY_t (W_t), the binary outcome, indicating whether death has occured within a given time window [t, t+ \\delta_d](which length \\delta_d is again chosen)"
  },
  {
    "objectID": "mydocs/technical_report/index.html#estimating-the-cate",
    "href": "mydocs/technical_report/index.html#estimating-the-cate",
    "title": "G4H - From embeddings to causal inference",
    "section": "2.2 Estimating the CATE",
    "text": "2.2 Estimating the CATE\nWe want to estimate the Conditional Average Treatment Effect (CATE):\n \\tau(x, w) \\coloneqq \\mathbb{E}[Y_t \\mid X_{&lt;t} = x, W_t = w] \n\n2.2.1 Assumptions\n\nSUTVA : Y_t(w) = Y_t \\quad \\textit{ if } W_t = w\n\nThe others not having a PCP for a given amount of time does not influence my probability to die\nOnly one version of treatment ?\n\nConditional ignorability\n\n \\forall w \\in \\mathbb{R}_{+}, Y_t(w) \\perp W_t \\mid X_{&lt;t}\n\nOverlap\n\n 0 &lt; \\mathbb{P}(W_t = w | X_{&lt;t} = x) &lt; 1, \\forall (w,x) \\in (\\mathbb{R}_{+}, \\mathbb{R}^{d_{embed}})\n\n\n2.2.2 Estimators\nFor a given time t, we define {pos}_t as being the last position in the patient’s pathway before t, and we define the embedding of the pathway:\n GPT^{\\text{path}}(X_{&lt;t}) \\coloneqq GPT_{\\theta^{*}}(X_{&lt;t})_{{pos}_t} \\in \\mathbb{R}^{d_{embed}} \n\nS-learner & T-learner\nThe S-learner is based on “outcome models” that try to capture the link between the outcome and the features (Abécassis et al. (2025)).\nMore precisely, we train on the validation set a head g_{\\phi}: \\mathbb{R}^{d_{embed}} \\times \\mathbb{R}_{+}  \\to [0,1] minimizing the binary cross-entropy loss on death prediction - to obtain \\phi^{*}6. At the end of the day:\n \\hat{\\tau}_{\\text{SLearner}}(x, w) \\coloneqq g_{\\phi^{*}} (GPT^{\\text{path}}(x), w) - g_{\\phi^{*}} (GPT^{\\text{path}}(x), 0)\nwhere the baseline is “not having lost your PCP / having a PCP during all the period”.\n🔴 There is no T-learner for continuous treatment, a priori\n\n\nR-learner\nAnother approach consists in trying to model the propensity score, the probability of being treated given X.\nA first head g_{\\phi_{m}}: \\mathbb{R}^{d_{embed}} \\to [0,1] is trained on the death prediction task, so that for a given pathway x, g_{\\phi_{m}^{*}}(GPT^{\\text{path}}(x)) is close to the conditional mean outcome m(x) = \\mathbb{E}[Y_t | x].\nAnother head g_{\\phi_{e}}: \\mathbb{R}^{d_{embed}} \\to \\mathbb{R}_+ is trained to predict the treatment W_t of a given individual (minimizing a Mean Squared Error).\nFinally, we train the CATE estimator training a head g_{\\phi_{\\tau}}: \\mathbb{R}^{d_{embed}} \\to [0,1] by minimizing the R-loss:\n\\begin{align*}\n\\phi_{\\tau}^{*} \\in \\arg\\!\\min_{\\phi_{\\tau}} \\; &\nY_t - g_{\\phi_{m}^{*}}\\left( GPT^{\\text{path}}(X_{&lt;t}) \\right) \\\\\n& - \\left( W_t - g_{\\phi_{e}^{*}}\\left( GPT^{\\text{path}}(X_{&lt;t}) \\right) \\cdot g_{\\phi_{\\tau}}\\left( GPT^{\\text{path}}(X_{&lt;t}) \\right) \\right)\n\\end{align*}\n🔴 There is no DR-learner for continuous treatment, a priori"
  },
  {
    "objectID": "mydocs/technical_report/index.html#sec-benchmarks",
    "href": "mydocs/technical_report/index.html#sec-benchmarks",
    "title": "G4H - From embeddings to causal inference",
    "section": "2.3 Benchmarks",
    "text": "2.3 Benchmarks\nWIP\n\nReferences\n\n\nAbécassis, Judith, Élise Dumas, Julie Alberge, and Gaël Varoquaux. 2025. “From prediction to prescription: Machine learning and Causal Inference.” Annual Review of Biomedical Data Science, April. https://doi.org/10.1146/annurev-biodatasci-103123-095750."
  },
  {
    "objectID": "mydocs/technical_report/index.html#footnotes",
    "href": "mydocs/technical_report/index.html#footnotes",
    "title": "G4H - From embeddings to causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nActually, we consider several tensors (and several vocabulary sizes) - one for each type of tokens, but we do not get into the details here.↩︎\nShorter sequences are padded, longer are cropped.↩︎\nTo describe a date, we actually use the number of days since 01/01/2016, the start of the available dataset↩︎\nThe rationale for having a causal mask is that we want to be able to exploit any embedding at any time (with only one inference pass through the model) without them having “seen” the future. Typically, the pre-training task (see Section 1.2.2) is such a task. However, we still plan to train a BERT - pre-training task is still in discussion (see Section 2.3).↩︎\nFor each short term horizon t, we crop the pathways t before the end date of our dataset so that we can always compute a ground truth↩︎\nIn this formulation, we assume that the embeddings are frozen (we only train \\phi at \\theta^{*} constant). Another alternative is to fine-tune the Transformer model, training both \\phi and \\theta here, with the same task, same loss, same dataset. This is true for all the remaining of this document.↩︎"
  }
]