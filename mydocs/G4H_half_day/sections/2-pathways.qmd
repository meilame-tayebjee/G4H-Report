## Our data {.unnumbered}

::: {.incremental}
- Our data: patient (medical) pathways
- Very similar to NLP, with differences
  - unstructured (just as in NLP)
  - temporally irregular (not like NLP!)

![](../csi_slides/images/pathway_data.png){fig-align="center"}
:::

## Embeddings {.unnumbered}

::: {.incremental}
- Represent tokens and patient as a **dense vector**, to:
  - make visualizations
  - use the vector for downstream task
    - prediction, classification
    - clustering
    - causal inference
:::

## Tokenization {.unnumbered}

**Dataset of medical pathways: ** For $i = 1, \dots, n$, $X_i \coloneqq (e_i, t_i, f_i)$, where:

::: {.incremental}
-  $e_i \in \mathbb{R}^{cs}$ are the tokenized events,  containing integers between $0$ and the vocabulary size $\lvert \mathcal{V} \rvert$
- $t_i \in \mathbb{R}^{cs}$ is the temporal vector, containing the dates
- $f_i \in \mathbb{R}^{2}$ corresponds to the patient general features, for now age and gender
:::

![](../csi_slides/images/tokenized_data.png){fig-align="center"}

## Pre-training task {.unnumbered}

### Objectives {.unnumbered}

:::{.incremental}
- Having a **time-aware** Transformers ($\neq$ NLP)
- Keeping in mind causal inference
  - We want to be able to efficiently compute the embedding of the pathway **at any time** (the *trajectory* of the patient)

- **Problems with the next token task:**
  - a lot of simultaneous events, especially for medications
  - different types of events: does it make sense to have a competition among them ?
:::


---

### Description {.unnumbered}

We predict for each token, if it will appear within the next $t$ days.

 We consider, simulatenously, several $t \in \mathcal{T}$, $\mathcal{T}$ being the sorted set of ***short term horizons***, typically 14, 30, 90, 180 days.

![](../csi_slides/images/GPT-pretraining.png)

## First trained models {.unnumbered}


We trained a "large" model (40M params) and a "small" one (5M).

![ROC curve of the pre-training task for a given specialty on a test set of 10,000 patients.](../technical_report/images/GPT_results/roc_spe_3.png)

## Plot of the embedding space {.unnumbered}

![](../csi_slides/images/embedding_space.png){fig-align="center"}