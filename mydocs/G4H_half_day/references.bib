
ï»¿@Article{Shmatko2025,
author={Shmatko, Artem
and Jung, Alexander Wolfgang
and Gaurav, Kumar
and Brunak, S{\o}ren
and Mortensen, Laust Hvas
and Birney, Ewan
and Fitzgerald, Tom
and Gerstung, Moritz},
title={Learning the natural history of human disease with generative transformers},
journal={Nature},
year={2025},
month={Sep},
day={17},
abstract={Decision-making in healthcare relies on understanding patients' past and current health states to predict and, ultimately, change their future course1--3. Artificial intelligence (AI) methods promise to aid this task by learning patterns of disease progression from large corpora of health records4,5. However, their potential has not been fully investigated at scale. Here we modify the GPT6 (generative pretrained transformer) architecture to model the progression and competing nature of human diseases. We train this model, Delphi-2M, on data from 0.4{\thinspace}million UK Biobank participants and validate it using external data from 1.9{\thinspace}million Danish individuals with no change in parameters. Delphi-2M predicts the rates of more than 1,000 diseases, conditional on each individual's past disease history, with accuracy comparable to that of existing single-disease models. Delphi-2M's generative nature also enables sampling of synthetic future health trajectories, providing meaningful estimates of potential disease burden for up to 20 years, and enabling the training of AI models that have never seen actual data. Explainable AI methods7 provide insights into Delphi-2M's predictions, revealing clusters of co-morbidities within and across disease chapters and their time-dependent consequences on future health, but also highlight biases learnt from training data. In summary, transformer-based models appear to be well suited for predictive and generative health-related tasks, are applicable to population-scale datasets and provide insights into temporal dependencies between disease events, potentially improving the understanding of personalized health risks and informing precision medicine approaches.},
issn={1476-4687},
doi={10.1038/s41586-025-09529-3},
url={https://doi.org/10.1038/s41586-025-09529-3}
}

@inproceedings{
steinberg2024motor,
title={{MOTOR}: A Time-to-Event Foundation Model For Structured Medical Records},
author={Ethan Steinberg and Jason Alan Fries and Yizhe Xu and Nigam Shah},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=NialiwI2V6}
}

@misc{chinchilla,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}