
@article{abecassis:hal-04774700,
  TITLE = {{From prediction to prescription: Machine learning and Causal Inference}},
  AUTHOR = {Ab{\'e}cassis, Judith and Dumas, {\'E}lise and Alberge, Julie and Varoquaux, Ga{\"e}l},
  URL = {https://hal.science/hal-04774700},
  JOURNAL = {{Annual Review of Biomedical Data Science}},
  PUBLISHER = {{Annual Review}},
  YEAR = {2025},
  MONTH = Apr,
  DOI = {10.1146/annurev-biodatasci-103123-095750},
  KEYWORDS = {personalized medicine ; data-driven decision making ; large scale data ; causal inference ; machine learning},
  PDF = {https://hal.science/hal-04774700v1/file/CausalReview.pdf},
  HAL_ID = {hal-04774700},
  HAL_VERSION = {v1},
}

ï»¿@Article{Shmatko2025,
author={Shmatko, Artem
and Jung, Alexander Wolfgang
and Gaurav, Kumar
and Brunak, S{\o}ren
and Mortensen, Laust Hvas
and Birney, Ewan
and Fitzgerald, Tom
and Gerstung, Moritz},
title={Learning the natural history of human disease with generative transformers},
journal={Nature},
year={2025},
month={Sep},
day={17},
abstract={Decision-making in healthcare relies on understanding patients' past and current health states to predict and, ultimately, change their future course1--3. Artificial intelligence (AI) methods promise to aid this task by learning patterns of disease progression from large corpora of health records4,5. However, their potential has not been fully investigated at scale. Here we modify the GPT6 (generative pretrained transformer) architecture to model the progression and competing nature of human diseases. We train this model, Delphi-2M, on data from 0.4{\thinspace}million UK Biobank participants and validate it using external data from 1.9{\thinspace}million Danish individuals with no change in parameters. Delphi-2M predicts the rates of more than 1,000 diseases, conditional on each individual's past disease history, with accuracy comparable to that of existing single-disease models. Delphi-2M's generative nature also enables sampling of synthetic future health trajectories, providing meaningful estimates of potential disease burden for up to 20 years, and enabling the training of AI models that have never seen actual data. Explainable AI methods7 provide insights into Delphi-2M's predictions, revealing clusters of co-morbidities within and across disease chapters and their time-dependent consequences on future health, but also highlight biases learnt from training data. In summary, transformer-based models appear to be well suited for predictive and generative health-related tasks, are applicable to population-scale datasets and provide insights into temporal dependencies between disease events, potentially improving the understanding of personalized health risks and informing precision medicine approaches.},
issn={1476-4687},
doi={10.1038/s41586-025-09529-3},
url={https://doi.org/10.1038/s41586-025-09529-3}
}

@article{life2vec,
  title = {Using Sequences of Life-Events to Predict Human Lives},
  author = {Savcisens, Germans and {Eliassi-Rad}, Tina and Hansen, Lars Kai and Mortensen, Laust Hvas and Lilleholt, Lau and Rogers, Anna and Zettler, Ingo and Lehmann, Sune},
  year = {2023},
  month = dec,
  journal = {Nature Computational Science},
  volume = {4},
  number = {1},
  pages = {43--56},
  issn = {2662-8457},
  doi = {10.1038/s43588-023-00573-5},
  urldate = {2025-09-30},
  langid = {english},
  file = {/Users/Meilame/Zotero/storage/4Z6QQHF7/Savcisens et al. - 2023 - Using sequences of life-events to predict human lives.pdf}
}

@article{yangTransformEHRTransformerbasedEncoderdecoder2023,
  title = {{{TransformEHR}}: Transformer-Based Encoder-Decoder Generative Model to Enhance Prediction of Disease Outcomes Using Electronic Health Records},
  shorttitle = {{{TransformEHR}}},
  author = {Yang, Zhichao and Mitra, Avijit and Liu, Weisong and Berlowitz, Dan and Yu, Hong},
  year = {2023},
  month = nov,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {7857},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43715-z},
  urldate = {2025-05-09},
  abstract = {Abstract                            Deep learning transformer-based models using longitudinal electronic health records (EHRs) have shown a great success in prediction of clinical diseases or outcomes. Pretraining on a large dataset can help such models map the input space better and boost their performance on relevant tasks through finetuning with limited data. In this study, we present TransformEHR, a generative encoder-decoder model with transformer that is pretrained using a new pretraining objective---predicting all diseases and outcomes of a patient at a future visit from previous visits. TransformEHR's encoder-decoder framework, paired with the novel pretraining objective, helps it achieve the new state-of-the-art performance on multiple clinical prediction tasks. Comparing with the previous model, TransformEHR improves area under the precision--recall curve by 2\% (               p               \,{$<$}\,0.001) for pancreatic cancer onset and by 24\% (               p               \,=\,0.007) for intentional self-harm in patients with post-traumatic stress disorder. The high performance in predicting intentional self-harm shows the potential of TransformEHR in building effective clinical intervention systems. TransformEHR is also generalizable and can be easily finetuned for clinical prediction tasks with limited data.},
  langid = {english},
  file = {/Users/Meilame/Zotero/storage/3W2PI88H/Yang et al. - 2023 - TransformEHR transformer-based encoder-decoder generative model to enhance prediction of disease ou.pdf}
}

@book{wagerCausalInferenceStatistical,
  title = {Causal {{Inference}}: {{A Statistical Learning Approach}}},
  author = {Wager, Stefan},
  year = {2024},
}
